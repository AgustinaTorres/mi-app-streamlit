{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, pipeline, AutoTokenizer\n",
    "from IPython.display import display, Markdown\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the text_generated\n",
    "#folder_path = 'processed_data'\n",
    "#file_path = os.path.join(folder_path, 'pdf2_text.txt')\n",
    "#file_path = os.path.join(folder_path, 'pdf3_text.txt')\n",
    "#file_path = os.path.join(folder_path, 'video_text.txt')\n",
    "#file_path = os.path.join(folder_path, 'scrapping_text.txt')\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    full_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print function\n",
    "def show_full_text(text):\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO T5 BASE\n",
    "(problema porque me corta las oraciones - truncado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Agustina Torres\\Documents\\DEV\\IMF\\TFM\\SELENIUM\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "en Europao.4(4):263-264 263 71. doi:10.1016/j.4(4)4(4)4(4):263- 71  del abuso y depen- dencia de sustancias en todos los pases europeos... - -. -. -4:2 2 , y los trabajos sobre la psicopatologa comórbida, con influencias socioculturales, incluyendo los diferentes contextos legales. Esto nos permitirá ayudar a la integración de la Unión Europea, por ejemplo, entre las cien- tficas de áreas básicas el Colegio Europeo de Ciencias de las Adic- tions. Ayudara a su vez a crear un marco de trabajo ade- cuado que facilita la investigación multinacional. A. Invitamos a todos los cientficos que tienen en las áreas relacionadas con los trastornos adictivos. a la práctica. — • Facilitación de la transferencia de la evidencia existente. B. Mejorar los servicios terapéuticos para los tras- tornos adictivos en Europa: — Promover la investigación sobre el coste-effecti- vidad de las intervenciones. . — Promoverá la investigación para que los pro- gramas y servicios de tratamiento dispongan de sistemas de evaluación, de procesos, y resulta- dos, que permitan determinar la efectividad y eficiencia de las intervenciones que desarrollen. (United Kingdom) Autores Sociedad Espaola de Toxicomanas C/ San Vicente, 112-Pta. 2 46007 Valencia Tel. : 96 352 15 48 e-mail: valderra@setox.org http://www.setoxx.org www.essa.org/ apoyar y esti- mular esta discusión. Especialmente en el orgorgorgorgorgorgorgorgorgorgorgorg.org.orgorgorgorgorgorgorgorgorgorgorgorg or orgorgorgorgorgorgorgorgorgorgorgorgorgorgorgorgorgorgorgorgorgorg/orgorgorgorgorgorgorgorgorgorgorgorgorgorg is a non-profit organization that aims to provide a safe and secure environment for"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class T5Summarizer:\n",
    "    def __init__(self, model_name='t5-base', max_chunk_length=512):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.max_chunk_length = max_chunk_length\n",
    "\n",
    "    def chunk_text(self, text):\n",
    "        # Tokenizar el texto y dividirlo en chunks según el máximo de tokens permitidos\n",
    "        tokens = self.tokenizer.encode(text, return_tensors=\"pt\", truncation=False)\n",
    "        chunk_size = self.max_chunk_length\n",
    "        \n",
    "        # Dividir los tokens en chunks\n",
    "        chunks = [tokens[0][i:i + chunk_size] for i in range(0, tokens.shape[1], chunk_size)]\n",
    "        return chunks\n",
    "\n",
    "    def summarize_chunk(self, chunk, max_length=256, num_beams=5, length_penalty=2.0, no_repeat_ngram_size=3):\n",
    "        # Generar el resumen de un chunk\n",
    "        summary_ids = self.model.generate(\n",
    "            chunk.unsqueeze(0),\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        # Decodificar el resumen\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "    def summarize(self, text, max_length=256, num_beams=5, length_penalty=2.0, no_repeat_ngram_size=3):\n",
    "        # Dividir el texto en chunks\n",
    "        chunks = self.chunk_text(text)\n",
    "        \n",
    "        # Resumir cada chunk y concatenar los resultados\n",
    "        summaries = [self.summarize_chunk(chunk, max_length, num_beams, length_penalty, no_repeat_ngram_size) for chunk in chunks]\n",
    "        \n",
    "        # Unir los resúmenes de los chunks\n",
    "        return \" \".join(summaries)\n",
    "    \n",
    "\n",
    "# Ejemplo de uso:\n",
    "summarizer = T5Summarizer()\n",
    "summary = summarizer.summarize(full_text)\n",
    "\n",
    "show_full_text(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO T5 BASE PERO CON AJUSTES EN LA DIVISION DE CHUNKS, POR ORACIONES.\n",
    "(problemas con los videos sin puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "la prevalencia del abuso y depen- dencia ofrecen sustancias en todos los pases europeo- sade. la investigación de los trastornos - no están suficientemente integrados dentro de la práctica clnica e las decisi- nos polticas. por otra parte se estan produciendo importante n   ­ gra  re- de, l--. h w  t enfoque global incluira, por ejemplo, los nuevos tra- bajos de investigación de genética molecular. y la interacción entre estos factores individuale- s con influencias socioculturale. no reem- plazar o duplicar su trabajo, teniendo a ampliar la Unión Europea; entre la epidemiol n    ­ gra - de-- de- in h i  de de \" versiune , la necesidad de un «Colegio Europeo de Ciencias de las Adicciones» («European College on Addiction Sciences») para establecer un foro que facilite y potencie el diálogo con otras organizacio- nes semblables tanto en Europa como fuera de ella. ayudara al cientficos que trabajan entre la versiune  ­ n gra­­-­­n­s - - ( h l  , el Colegio Européen de Trastornos Adictivos celebra los próximos 27 a 29 de marzo de 2003 en Alicante (Espaa). entre los objeti- vo- siguientes: A. Mejorar la contribución de la investigación. B. Promover un abordaje basado à la videncia de los intervenciones, mediante las : — Identificación.  ­­ n­-­ de  h - (  t,  s-- de--. c. Mejorar los servicios terapéuticos para los tras- tornos adictivos en Europa. d. Promoverá más investigación para poder pro- porcionar nueva opciones áreas de evaluación, de proceso y resulta- dos, que permitan determinar la efficiencia de las intervenciones que desarrollen. la capacidad de electra de los paciente  - gra ­ n l   versiune  h, s- de de de in  de el i Symposium Europeo sobre Proble- mas Clnicos relacionados con los Trastornos Adicti- vos desean expresar su agradecimiento al Plan de Ga-licia. la sociedad espaola de Toxicomanas por apoyar y esti-mular esta discusión. los participantes reunidos en Santiago de Compostela, gra   ­­ n­-­n  versiune - de  o l--. h t s marc Auriacombe (France), Manuel Arajo (Spain) and indalecio Carrera (Ireland) e-mail: valderra@setox.org 96 352 15 48. cda: http://www.cochrane drugs and alcohol review group; swiss: www.icromaremmani (italy and EuroPAT); márquez, lvarez,s n - ­­ l   re gra versiune,--..-.s-s ("
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class T5Summarizer:\n",
    "    def __init__(self, model_name='t5-base', max_chunk_length=512):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.max_chunk_length = max_chunk_length\n",
    "\n",
    "    def split_text_into_chunks(self, text):\n",
    "        # Dividir el texto en oraciones\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        chunks = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Añadir la oración al chunk actual\n",
    "            current_chunk += sentence + \" \"\n",
    "            tokenized_chunk = self.tokenizer.encode(current_chunk, return_tensors=\"pt\", truncation=False)\n",
    "\n",
    "            # Si el chunk tokenizado excede el tamaño máximo permitido, guardarlo y empezar uno nuevo\n",
    "            if tokenized_chunk.shape[1] > self.max_chunk_length:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"  # Iniciar nuevo chunk con la oración actual\n",
    "\n",
    "        # Añadir el último chunk si no está vacío\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def summarize_chunk(self, chunk, max_length=400, min_length=150, num_beams=4, length_penalty=1.2, no_repeat_ngram_size=2):\n",
    "        inputs = self.tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\", max_length=self.max_chunk_length, truncation=True)\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=num_beams,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "    def summarize(self, text, max_length=400, min_length=150, num_beams=4, length_penalty=1.2, no_repeat_ngram_size=2):\n",
    "        # Dividir el texto en chunks respetando oraciones\n",
    "        chunks = self.split_text_into_chunks(text)\n",
    "        \n",
    "        # Resumir cada chunk\n",
    "        summaries = [self.summarize_chunk(chunk, max_length, min_length, num_beams, length_penalty, no_repeat_ngram_size) for chunk in chunks]\n",
    "        \n",
    "        # Post-procesamiento: unir los resúmenes con transiciones suaves\n",
    "        full_summary = \" \".join(summaries)\n",
    "        return full_summary\n",
    "\n",
    "# Uso del modelo\n",
    "summarizer = T5Summarizer()\n",
    "\n",
    "# Resumir el texto\n",
    "summary = summarizer.summarize(full_text)\n",
    "\n",
    "\n",
    "show_full_text(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO MT5 - MEJOR Y MULTILINGUAL\n",
    "(ANDA PESIMO!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<extra_id_0> la investigación de los trastornos adictivos, <extra_id_1> en el contexto europeo, y en el desarrollo de la sociedad en todo el mundo. P.P.. <extra_id_2>,.. <extra_id_3>.. <extra_id_12>.... <extra_id_4>.. <extra_id_5>. <extra_id_6>. <extra_id_7>. <extra_id_8>. <extra_id_9>. <extra_id_10>. <extra_id_11> Trastornos Adictivos en Europa <extra_id_12>.. <extra_id_13>. <extra_id_14>. <extra_id_15>.. <extra_id_16>.. <extra_id_17>. Trastorno adictivo en Europa Trastorno <extra_id_18> s <extra_id_0> a la investigación y la comunicación: <extra_id_1> de acuerdo a los objetivos del Colegio Europeo de Ciencias de las Adic- ciones <extra_id_2>.. <extra_id_3>.. <extra_id_4>.. <extra_id_5>.. <extra_id_6>.. a <extra_id_7>.. <extra_id_8>.. <extra_id_9>.  <extra_id_10>. e <extra_id_11>. <extra_id_12>. Adicciones C <extra_id_13>. <extra_id_14>. <extra_id_15>. Adic <extra_id_16>  <extra_id_54>...... <extra_id_18>... <extra_id_18>.. <extra_id_18> de adicciones.. <extra_id_0> de la Sociedad Española de Toxicomanías PSICOLOGÍA <extra_id_1> NOTICIAS Trastornos Adictivos 2002;4(4):263-264 72  <extra_id_2> A <extra_id_3> la investigación y la práctica, <extra_id_4> y  <extra_id_5> a <extra_id_6> a <extra_id_7> a <extra_id_8> a  <extra_id_9> a <extra_id_10> a <extra_id_11> a <extra_id_12> a <extra_id_13>: <extra_id_17> Política y Clínica <extra_id_18>, <extra_id_20>, <extra_id_21> a <extra_id_22> a <extra_id_23> i <extra_id_24>  <extra_id_25>  <extra_id_26>s <extra_id_27>es Pacientes adictivos Trastorno a <extra_id_28>  <extra_id_29>  <extra_id_29> patologías psiquiátricas y somáticas <extra_id_30>es <extra_id_31>es  <extra_id_32>es  <extra_id_33>es  <extra_id_34> i <extra_id_35> <extra_id_0> (Spain) Francisco Javier Auriacombe (Belgium/ <extra_id_1>) José Carlos Pérez de los Cobos (España) Todos los participantes de esta discusión, y  <extra_id_2>..,... <extra_id_3>..., <extra_id_4>..),... <extra_id_5>... <extra_id_6>  <extra_id_13>... <extra_id_14>... <extra_id_15>... <extra_id_16>... <extra_id_17>..  <extra_id_18>.. <extra_id_18>....... <extra_id_19>  <extra_id_20>  <extra_id_21>s <extra_id_22>...,...  <extra_id_23> Alcohol Review Group <extra_id_41> <extra_id_54>...... <extra_id_0> (Spain) José María Valdérraga (Spain), <extra_id_1> (Italy and EuroPAT) Participants: <extra_id_2> (Spain): <extra_id_3> (Spanish): <extra_id_12> <extra_id_53>, <extra_id_56> (Spanish: Sociedad Española de Toxicomanías C/ San Vicente) 112-Pta. 2 46007 Valencia Tel: 96 352 15 48 e-mail: valderra@setox.org http://www.setoxgroup.org/en/ -... -.... -.. )"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class T5Summarizer:\n",
    "    def __init__(self, model_name='google/mt5-base', max_chunk_length=512):\n",
    "        self.tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.max_chunk_length = max_chunk_length\n",
    "    \n",
    "    def split_text_into_chunks(self, text):\n",
    "        # Dividir el texto en oraciones\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        chunks = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Añadir la oración al chunk actual\n",
    "            current_chunk += sentence + \" \"\n",
    "            tokenized_chunk = self.tokenizer.encode(current_chunk, return_tensors=\"pt\", truncation=False)\n",
    "\n",
    "            # Si el chunk tokenizado excede el tamaño máximo permitido, guardarlo y empezar uno nuevo\n",
    "            if tokenized_chunk.shape[1] > self.max_chunk_length:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"  # Iniciar nuevo chunk con la oración actual\n",
    "\n",
    "        # Añadir el último chunk si no está vacío\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def summarize_chunk(self, chunk, max_length=300, min_length=100, num_beams=6, length_penalty=2.0, no_repeat_ngram_size=3):\n",
    "        # Agregar el prefijo \"summarize: \" antes del chunk\n",
    "        inputs = self.tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\", max_length=self.max_chunk_length, truncation=True)\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=num_beams,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "\n",
    "    def summarize(self, text, max_length=300, min_length=100, num_beams=6, length_penalty=2.0, no_repeat_ngram_size=3):\n",
    "        # Dividir el texto en chunks respetando oraciones\n",
    "        chunks = self.split_text_into_chunks(text)\n",
    "        \n",
    "        # Resumir cada chunk\n",
    "        summaries = [self.summarize_chunk(chunk, max_length, min_length, num_beams, length_penalty, no_repeat_ngram_size) for chunk in chunks]\n",
    "        \n",
    "        # Post-procesamiento: unir los resúmenes con transiciones suaves\n",
    "        full_summary = \" \".join(summaries)\n",
    "        return full_summary\n",
    "\n",
    "# Uso del modelo\n",
    "summarizer = T5Summarizer()\n",
    "\n",
    "# Resumir el texto\n",
    "summary = summarizer.summarize(full_text)\n",
    "#print(summary)\n",
    "\n",
    "\n",
    "show_full_text(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO PEGASUS (INGLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Agustina Torres\\Documents\\DEV\\IMF\\TFM\\SELENIUM\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-multi_news and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "– The European Court of Human Rights has ruled in favor of a Spanish woman who says she was fired from her job as a teacher because she refused to give birth to a child, reports the Telegraph. The court in Strasbourg, France, said that the woman's complaint \"did not meet – The European Commission is calling for more research into the effects of alcohol on people's mental health in a bid to cut down on the number of people dying from alcohol-related diseases in Europe, the Telegraph reports. The number of alcohol-related deaths in Europe has more than doubled in – The Colegio Europeo de Ciencias de la Ciencias de las Adicciones, a European College of Addiction Sciences-supported program that helps people struggling with substance abuse, has been forced to close its doors because of a lack of funding, the BBC reports. The program – Spain's government has taken the unusual step of setting up a fund to compensate victims of the country's devastating recession, the Wall Street Journal reports. The 100 million ($140 million) fund will cover victims of the economic crisis who lost their jobs or suffered other hardships, – Spain is in the midst of an economic crisis, and the country's prime minister has called for an end to austerity measures, the BBC reports. Mariano Rajoy made the call at the opening of a two-day conference on economic policy in Santiago de Compostela, Spain, where he – A Spanish court has ordered the release of a man accused of killing his wife's best friend with a hammer in 2013, the AP reports. The court on Tuesday ordered the release of Francisco Javier lvarez Rodrguez after he pleaded not guilty to murder, attempted murder, and"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#google/pegasus-xsum\n",
    "#google/pegasus-large\n",
    "#oogle/pegasus-multi_news\n",
    "\n",
    "class PegasusSummarizer:\n",
    "    def __init__(self, model_name='google/pegasus-multi_news', max_chunk_length=512):\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        self.model = PegasusForConditionalGeneration.from_pretrained(model_name)  # Sin from_tf\n",
    "        self.max_chunk_length = max_chunk_length\n",
    "\n",
    "    def split_text_into_chunks(self, text):\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        chunks = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentence = self.tokenizer.encode(sentence, return_tensors=\"pt\", truncation=False)\n",
    "            current_length += len(tokenized_sentence[0])\n",
    "\n",
    "            if current_length > self.max_chunk_length:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "                current_length = len(tokenized_sentence[0])\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    def summarize_chunk(self, chunk, max_length=60, min_length=30, num_beams=4):\n",
    "        inputs = self.tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=\"longest\", max_length=self.max_chunk_length)\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    \n",
    "\n",
    "\n",
    "    def summarize(self, text, max_length=60, min_length=30, num_beams=4):\n",
    "        chunks = self.split_text_into_chunks(text)\n",
    "        summaries = [self.summarize_chunk(chunk, max_length, min_length, num_beams) for chunk in chunks]\n",
    "        full_summary = \" \".join(summaries)\n",
    "        return full_summary\n",
    "\n",
    "# Uso del modelo PEGASUS\n",
    "pegasus_summarizer = PegasusSummarizer()\n",
    "\n",
    "# Resumir el texto\n",
    "summary = pegasus_summarizer.summarize(full_text)\n",
    "#print(summary)\n",
    "\n",
    "show_full_text(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO MBART - MULTILINGUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Adictiva Transtornos 2002;4(4):263-264 263 71 «Documento Galicia sobre Problemas Relacionados con los Adictivos en Europa» There is an increase in the prevalence of abuse and addiction of substances in all European countries, and addictive disorders represent a serious problem for the affected individuals, their families and for society throughout Europe. La investigación de los tras- ctores en el contexto europeo debe conducir a una mejor comprensión del problema subyacente, al desarrollo de tratamientos más eficaces basa- dos en la evidencia científica; y ofrecer un modelo para un mejor integración de las diferentes perspectivas que influyen sobre los diferentes aspectos individuales, de desarrollo, social y cultural en Europa. Este enfoque global incluiría, por ejemplo, los nuevos tra- encos de genes moleculares, neurobiológicos y psychologicos, trabajos sobre la psicopatología combinada, y la interacción de estos factores individuales con influencias socioculturales including los distintos contextos legales. Por el contrario, nuestra intención es añadir una dimensión europea más integrada, teniendo también en cuenta la ampliación de la Unión Europea, y servir como foro que albergue la comunicación y el enriquecimiento mutuo entre cien médicos de áreas básicas y clínicas; entre el epidemio- gía y la salud pública, para ayudar a la planificación de políticas de intervención, lo que nos permitirá ir en contra de las diferencias y las similitudes no sólo entre las drogas, el alcohol e el tabaco, sino también entre los diferentes países europeos, que en su vez facilitaría las colaboraciones de investigación entre grupos de distintos países y permitiría la elaboración de guías e protocolos terapéuticos eficaces y prácticos. En este contexto, el Ministerio de Salud y Bienestar Social, en colaboración con la Oficina del Alto Comisionado de las Naciones Unidas para los Derechos Humanos (ACNUDH), organizó un seminario sobre la salud de los adolescentes en el que participaron representantes de la sociedad civil, los medios de comunicación, las organizaciones no gubernamentales (ONG) y el sector privado, con el fin de examinar las cuestiones relacionadas con los niños y los jóvenes. Envió una carta al Secretario General de las Naciones Unidas sobre el VIH/SIDA, en la que se pedía a la Oficina del Alto Comisionado para los Derechos Humanos (ACNUDH) que prestara asistencia a los gobiernos y a las organizaciones no gubernamentales (ONG) en relación con el problema de la violencia contra la mujer. b) Establezca una red de expertos que podrían ex- plorar las oportunidades de apoyo económico en Europa que faciliten las iniciativas comunes de investigación multi-nacional y interdisciplinar. B. Improve the therapeutic services for las tras- rounds adictivas en Europe: — Tanto en atención primaria como especializada la base de evidencia científica de los enfoques terapéuticos no siempre es suficiente. El Presidente (habla en inglés): De conformidad con el entendimiento alcanzado en las consultas previas, entenderé que la Asamblea General desea aprobar el proyecto de resolución A/C.2/57/L.15/Rev.1, titulado “Convención de las Naciones Unidas contra el Tráfico Ilícito de Estupefacientes y Sustancias Sicotrópicas de 1988”. A/C.2/57/L.18 Temas a) y b) del programa - Informe del Secretario General sobre la situación de los derechos humanos y las libertades fundamentales en la República Democrática del Congo - Proyecto de resolución presentado por el Presidente del Consejo de Seguridad [A C E F I R] En el párrafo 2 de la parte dispositiva del proyecto de presupuesto por programas para el bienio 2006-2007, la Junta recomendó que el Secretario General presentara un informe a la Asamblea General en su quincuagésimo séptimo período de sesiones sobre la situación de los derechos humanos en la República Democrática del Congo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MultilingualSummarizer:\n",
    "    source_lang = 'es_XX'  # español\n",
    "    target_lang = 'es_XX'   # también español\n",
    "\n",
    "    def __init__(self, model_name='facebook/mbart-large-50-many-to-many-mmt'):\n",
    "        self.tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "        self.model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def summarize(self, text, source_lang = source_lang, target_lang = target_lang, max_length=512, num_beams=5, length_penalty=1.5, no_repeat_ngram_size=2):\n",
    "        self.tokenizer.src_lang = source_lang\n",
    "        forced_bos_token_id = self.tokenizer.lang_code_to_id[target_lang]\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "        summary_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            forced_bos_token_id=forced_bos_token_id,\n",
    "            num_beams=num_beams,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            early_stopping=False\n",
    "        )\n",
    "\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "#def preprocess_text(text):\n",
    "    # Remove irrelevant mentions and other noise\n",
    "    #clean_text = re.sub(r'\\[Música\\].*?Compartir', '', text, flags=re.DOTALL).strip()\n",
    "    # Ensure consistent punctuation and spacing\n",
    "   #clean_text = re.sub(r'\\s+', ' ', clean_text)  # Replace multiple spaces with a single space\n",
    "    #return clean_text\n",
    "\n",
    "def split_text(text, max_length):\n",
    "    sentences = text.split('. ')  # Use period as delimiter\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1 > max_length:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += ('. ' if current_chunk else '') + sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "def summarize_text(text, summarizer, max_length=512, chunk_size=1024):\n",
    "    chunks = split_text(text, chunk_size)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer.summarize(chunk, max_length=max_length)\n",
    "        summaries.append(summary)\n",
    "    return ' '.join(summaries)\n",
    "\n",
    "# Usage in the code\n",
    "summarizer = MultilingualSummarizer()\n",
    "#input_text = preprocess_text(full_text)\n",
    "summarized_text = summarize_text(full_text, summarizer)\n",
    "\n",
    "show_full_text(summarized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO BART (INGLES) +  PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hay un aumento en la prevalencia del abuso de sustancias y la dependencia en todos los países europeos. Sin embargo, la investigación sigue siendo baja y hay una falta de comunicación entre los grupos de profesionales que trabajan en las diferentes áreas relacionadas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the translation models and summarization model\n",
    "translator_es_en = pipeline(\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "translator_en_es = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Load tokenizer for chunking\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Function to split the text into smaller chunks using the tokenizer\n",
    "def chunk_text(text, tokenizer, max_length=512):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    tokens = inputs['input_ids'].squeeze()\n",
    "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Translate the text into English\n",
    "    translated_text_chunks = chunk_text(text, tokenizer)\n",
    "    translated_text = \"\"\n",
    "\n",
    "    for chunk in translated_text_chunks:\n",
    "        translated_text += translator_es_en(chunk)[0]['translation_text'] + \" \"\n",
    "\n",
    "    # Split the translated text into smaller chunks if it's too long\n",
    "    text_chunks = chunk_text(translated_text.strip(), tokenizer)\n",
    "\n",
    "    # Generate the summary for each part of the text\n",
    "    summary = \"\"\n",
    "    for chunk in text_chunks:\n",
    "        input_length = len(chunk.split())\n",
    "        dynamic_max_length = min(max(30, input_length // 2), 150)  # Ajuste de max_length\n",
    "        summary_result = summarizer(chunk, max_length=dynamic_max_length, min_length=50, do_sample=False)  # Aumento de min_length\n",
    "        summary += summary_result[0]['summary_text'] + \" \"\n",
    "\n",
    "    # Translate the summary back into Spanish\n",
    "    final_summary_chunks = chunk_text(summary.strip(), tokenizer)\n",
    "    final_summary = \"\"\n",
    "\n",
    "    for chunk in final_summary_chunks:\n",
    "        final_summary += translator_en_es(chunk)[0]['translation_text'] + \" \"\n",
    "\n",
    "    return final_summary.strip()\n",
    "\n",
    "\n",
    "# Call the English model\n",
    "summarized_text = summarize_text(full_text)\n",
    "\n",
    "\n",
    "show_full_text(summarized_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
